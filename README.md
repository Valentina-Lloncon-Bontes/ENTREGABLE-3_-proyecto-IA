MODELO FUNCIONAL Y DEFINITIVO ELEGIDO
üîπ Nombre del sistema: EduIA ‚Äì Mentor Acad√©mico 24/7 con IA
üîπ Tipo de soluci√≥n: Chatbot acad√©mico con recuperaci√≥n de informaci√≥n basada en documentos PDF, enriquecido con embeddings sem√°nticos y una IA generativa para respuestas personalizadas.
‚úÖ JUSTIFICACI√ìN DE LA ELECCI√ìN DEL MODELO
Funcionalidad comprobada: El c√≥digo fue probado exitosamente, permitiendo responder preguntas espec√≠ficas sobre material educativo en formato PDF usando IA generativa local (Llama3 v√≠a Ollama).

Simplicidad escalable: La arquitectura inicial es modular y admite escalabilidad (agregando APIs, m√°s modelos, bases de datos, LMS, etc.).

Precisi√≥n sem√°ntica: Usa SentenceTransformer con paraphrase-multilingual-MiniLM-L12-v2, lo que permite interpretar la sem√°ntica del contenido educativo en espa√±ol y otras lenguas.

B√∫squeda eficiente: El √≠ndice FAISS permite recuperar r√°pidamente los fragmentos relevantes del contenido acad√©mico.

Respuestas relevantes: El modelo Llama3, al usar contexto, reduce el riesgo de "alucinaciones" y responde s√≥lo seg√∫n la informaci√≥n cargada (seguridad acad√©mica).

Alineaci√≥n con la visi√≥n de EduIA: El flujo IA-embedding-contexto-documentos se ajusta perfectamente a la idea de mentor√≠a personalizada, sin depender a√∫n de LMS o universidades, pero compatible con ese futuro.

‚úÖ ELEMENTOS DEL MODELO INTEGRADOS EN EL C√ìDIGO
A continuaci√≥n se detallan todas las conexiones, modelos, APIs y herramientas incluidas o simuladas en el c√≥digo script.py que componen el MVP:

üî∏ 1. Extracci√≥n de contenido desde PDFs
Librer√≠a: PyPDF2

Funci√≥n: extract_text_from_pdf()

Objetivo: Convertir m√∫ltiples PDFs en texto plano para ser usado como base de conocimiento.

üî∏ 2. Segmentaci√≥n de texto en chunks
Funci√≥n: split_text_into_chunks()

Par√°metros ajustables: max_chunk_size=500, overlap=50

Objetivo: Permitir mejor recuperaci√≥n contextual en la fase de embeddings y consulta.

üî∏ 3. Modelo de embeddings sem√°nticos
Librer√≠a: sentence-transformers

Modelo utilizado: 'paraphrase-multilingual-MiniLM-L12-v2'

Funci√≥n: model.encode(text_chunks)

Objetivo: Convertir texto en vectores que capturen el significado sem√°ntico del contenido.

üî∏ 4. Indexaci√≥n sem√°ntica con FAISS
Librer√≠a: faiss

√çndice: IndexFlatL2

Uso: faiss_index.add(...), luego index.search(...)

Objetivo: Buscar fragmentos m√°s similares sem√°nticamente al input del usuario.

üî∏ 5. Modelo generativo (LLM)
Interfaz: Ollama API (local)

Modelo LLM: "llama3" (deepseek/llama3 puede ajustarse)

Endpoint: http://localhost:11434/api/generate

Formato solicitud:

json
Copiar
Editar
{
  "model": "llama3",
  "prompt": "<prompt>",
  "stream": false
}
Funci√≥n encargada: call_ollama_deepseek()

üî∏ 6. Prompt estructurado y rol del sistema
Se define a EduIA como mentor acad√©mico especializado en psicolog√≠a educativa, coaching y vocaci√≥n, restringido a solo usar el contenido documental.

El prompt incluye: rol del sistema + contexto relevante (chunks) + pregunta del usuario.




FLUJO DE AUTOMATIZACI√ìN DEL SCRIPT.PY (DESCRIPCI√ìN COMPLETA)
A continuaci√≥n, el flujo funcional automatizado del script script.py:

less
Copiar
Editar
[INICIO SCRIPT]
        ‚îÇ
        ‚ñº
[CARGA Y EXTRACCI√ìN]
‚Üí Se define la carpeta de PDFs.
‚Üí Se recorre la carpeta y se extrae texto de cada PDF con PyPDF2.
‚Üí Si falla la extracci√≥n, el programa se detiene.

        ‚îÇ
        ‚ñº
[SEGMENTACI√ìN EN CHUNKS]
‚Üí El texto completo se divide en fragmentos de 500 caracteres con solapamiento.

        ‚îÇ
        ‚ñº
[EMBEDDINGS]
‚Üí Se carga el modelo de SentenceTransformer.
‚Üí Se generan embeddings para cada fragmento.
‚Üí Se guarda el √≠ndice FAISS (`pdf_embeddings.faiss`) y los chunks (`text_chunks.json`).

        ‚îÇ
        ‚ñº
[SE INICIA EL CHATBOT]
‚Üí `chat_with_eduaia()` solicita input del usuario en consola.

        ‚îÇ
        ‚ñº
[RESPUESTA EN TIEMPO REAL]
‚Üí Para cada pregunta:
    1. Se calcula su embedding.
    2. Se buscan los 3 chunks m√°s relevantes usando FAISS.
    3. Se construye el prompt completo para el modelo LLM (EduIA).
    4. Se hace una llamada POST a la API local de Ollama.
    5. Se muestra la respuesta generada.

        ‚îÇ
        ‚ñº
[LOOP hasta que el usuario escribe 'salir']


‚úÖ CONCLUSI√ìN
El modelo funcional y definitivo elegido es un sistema conversacional basado en recuperaci√≥n aumentada por IA (RAG). La elecci√≥n se justifica por su equilibrio entre robustez t√©cnica, personalizaci√≥n educativa, facilidad de uso y escalabilidad futura. El c√≥digo script.py implementa correctamente esta arquitectura, con todos los componentes fundamentales documentados,
