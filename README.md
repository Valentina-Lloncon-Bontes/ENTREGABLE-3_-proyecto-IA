MODELO FUNCIONAL Y DEFINITIVO ELEGIDO
üîπ Nombre del sistema: EduIA ‚Äì Mentor Acad√©mico 24/7 con IA
üîπ Tipo de soluci√≥n: Chatbot acad√©mico con recuperaci√≥n de informaci√≥n basada en documentos PDF, enriquecido con embeddings sem√°nticos y una IA generativa para respuestas personalizadas.
‚úÖ JUSTIFICACI√ìN DE LA ELECCI√ìN DEL MODELO
Funcionalidad comprobada: El c√≥digo fue probado exitosamente, permitiendo responder preguntas espec√≠ficas sobre material educativo en formato PDF usando IA generativa local (Llama3 v√≠a Ollama).

Simplicidad escalable: La arquitectura inicial es modular y admite escalabilidad (agregando APIs, m√°s modelos, bases de datos, LMS, etc.).

Precisi√≥n sem√°ntica: Usa SentenceTransformer con paraphrase-multilingual-MiniLM-L12-v2, lo que permite interpretar la sem√°ntica del contenido educativo en espa√±ol y otras lenguas.

B√∫squeda eficiente: El √≠ndice FAISS permite recuperar r√°pidamente los fragmentos relevantes del contenido acad√©mico.

Respuestas relevantes: El modelo Llama3, al usar contexto, reduce el riesgo de "alucinaciones" y responde s√≥lo seg√∫n la informaci√≥n cargada (seguridad acad√©mica).

Alineaci√≥n con la visi√≥n de EduIA: El flujo IA-embedding-contexto-documentos se ajusta perfectamente a la idea de mentor√≠a personalizada, sin depender a√∫n de LMS o universidades, pero compatible con ese futuro.

‚úÖ ELEMENTOS DEL MODELO INTEGRADOS EN EL C√ìDIGO
A continuaci√≥n se detallan todas las conexiones, modelos, APIs y herramientas incluidas o simuladas en el c√≥digo script.py que componen el MVP:

üî∏ 1. Extracci√≥n de contenido desde PDFs
Librer√≠a: PyPDF2

Funci√≥n: extract_text_from_pdf()

Objetivo: Convertir m√∫ltiples PDFs en texto plano para ser usado como base de conocimiento.

üî∏ 2. Segmentaci√≥n de texto en chunks
Funci√≥n: split_text_into_chunks()

Par√°metros ajustables: max_chunk_size=500, overlap=50

Objetivo: Permitir mejor recuperaci√≥n contextual en la fase de embeddings y consulta.

üî∏ 3. Modelo de embeddings sem√°nticos
Librer√≠a: sentence-transformers

Modelo utilizado: 'paraphrase-multilingual-MiniLM-L12-v2'

Funci√≥n: model.encode(text_chunks)

Objetivo: Convertir texto en vectores que capturen el significado sem√°ntico del contenido.

üî∏ 4. Indexaci√≥n sem√°ntica con FAISS
Librer√≠a: faiss

√çndice: IndexFlatL2

Uso: faiss_index.add(...), luego index.search(...)

Objetivo: Buscar fragmentos m√°s similares sem√°nticamente al input del usuario.

üî∏ 5. Modelo generativo (LLM)
Interfaz: Ollama API (local)

Modelo LLM: "llama3" (deepseek/llama3 puede ajustarse)

Endpoint: http://localhost:11434/api/generate

Formato solicitud:

json
Copiar
Editar
{
  "model": "llama3",
  "prompt": "<prompt>",
  "stream": false
}
Funci√≥n encargada: call_ollama_deepseek()

üî∏ 6. Prompt estructurado y rol del sistema
Se define a EduIA como mentor acad√©mico especializado en psicolog√≠a educativa, coaching y vocaci√≥n, restringido a solo usar el contenido documental.

El prompt incluye: rol del sistema + contexto relevante (chunks) + pregunta del usuario.
